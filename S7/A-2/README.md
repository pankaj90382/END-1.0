# Session 7 A-2 - Encoder Decoder LSTM Architecture on Multiple Data Sets

## Objective

1.  Train model with the help of Multi30K Dataset and additional Dataset from the following Links to train the Data seperately.
    1.  http://www.cs.cmu.edu/~ark/QA-data/
    2.  https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs

2.  Once done, please upload the file to GitHub and proceed to answer these questions in the S7 - Assignment Solutions, where these questions are asked:
    1.  Share the link to your GitHub repo.
    2.  Share the link to your readme file.
    3.  Copy-paste the code related to your dataset preparation for both datasets.

## Solution

**DataSets**|**Github**|**Colab**
:-----:|:-----:|:-----:
German to English Machine Translation|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/Class\_Code\_END2%20Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/Class\_Code\_END2%20Seq2seq%20Class%20Code.ipynb)
[Wikipedia question/answer pairs](http://www.cs.cmu.edu/~ark/QA-data/) from Carnegie Mellon University|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/QA\_Dataset\_Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/QA\_Dataset\_Seq2seq%20Class%20Code.ipynb)
[First Quora Dataset Release: Question Pairs](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/Quora\_Dataset\_Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/Quora\_Dataset\_Seq2seq%20Class%20Code.ipynb)

### Datasets

#### Wikipedia Question/Answer Pairs
The Question/Answer dataset generated by students who took undergraduate natural language processing courses taught by Noah Smith 
at Carnegie Mellon and Rebecca Hwa at the University of Pittsburgh during 
Spring 2008, Spring 2009, and Spring 2010.

There are three directories, one for each year of students: S08, S09, and S10 and each of the folder have question_answer_pairs.txt file

These file contains the totaly 6 columns out of which question and answer are our point of interset

Also note that, there are frequently multiple lines with the same question, which appear if those questions were answered 
by multiple individuals. So duplicate questions should be retained.

#### Multi30k

Multi30K dataset to stimulate multilingual multimodal research. The translations were collected from professional English-German translators contracted via an established Language Service in Germany. Translated description per image, resulting in a total of 31,014 translations. To ensure an even distribution over description length, the English descriptions were chosen based on their relative length, with an equal number of longest, shortest, and median length source descriptions.

#### Quora Dataset Release: Question Pairs
An important product principle for Quora is that there should be a single question page for each logically distinct question. As a simple example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.

The data released by quora conatins set of questions with alternative ways of asking the same questions along with tag which says wheather question is a duplicated question or not

Data set contains 6 columns out of which question1,question2 and is_duplicate fields are our interset. Only rows with is_duplicate =1 is considered

### Approach

**1.  Download and Preprocess**

-   Multi30 Dataset

    Direct from torchtext

    ```python
    from torchtext.legacy.datasets import Multi30k
    ```     
 -   Wikipedia Question/Answer Pairs

        ```python
        !wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz

        ! tar -xf /content/Question_Answer_Dataset_v1.2.tar.gz

        import glob

        import pandas as pd


        QA_dataset_complete = pd.DataFrame()

        # get all .txt files from all subdirectories

        all_files = glob.glob('./Question_Answer_Dataset_v1.2/*/*.txt')


        for file in all_files:

          QA_dataset = pd.read_csv(file, sep='\t', encoding= 'ISO-8859-1')
                                   #'unicode_escape',header=None,error_bad_lines=False,skiprows=[0])
          print(f'Complete file path :{file} | Number of records or dimentions : {QA_dataset.shape}')
          QA_dataset_complete=pd.concat([QA_dataset_complete,QA_dataset])


        print(f'After combining all files total number of records or dimentions : {QA_dataset_complete.shape}')
        ```  
-   Quora Dataset Release: Question Pairs

    ```python

    !wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv

    import pandas as pd

    quora_duplicate_ques_dataset = pd.read_csv("/content/quora_duplicate_questions.tsv", sep='\t')

    print(f' Total number of records or dimentions : {quora_duplicate_ques_dataset.shape}')

    quora_duplicate_ques_dataset = quora_duplicate_ques_dataset[quora_duplicate_ques_dataset['is_duplicate']==1]

    quora_duplicate_ques_dataset.reset_index(drop=True,  inplace=True)
    ``` 
    
**2.  Data Cleanning**

-   Multi30 Dataset

            There is no data cleanning required in Multi30k Dataset

-   Wikipedia Question/Answer Pairs

    ```python

    QA_dataset_complete.isnull().sum()
    ArticleTitle                  0
    Question                     37
    Answer                      576
    DifficultyFromQuestioner    955
    DifficultyFromAnswerer      580
    ArticleFile                   2
    dtype: int64

    QA_dataset_complete.dropna(subset=['Question','Answer'], inplace=True)
    QA_dataset_complete.shape
    (3422, 6)

    QA_dataset_complete.isnull().sum()

    ArticleTitle                  0
    Question                      0
    Answer                        0
    DifficultyFromQuestioner    688
    DifficultyFromAnswerer        5
    ArticleFile                   2
    dtype: int64
    ```

-   Quora Dataset Release: Question Pairs

    ```python
    quora_duplicate_ques_dataset.isnull().sum()

    id              0
    qid1            0
    qid2            0
    question1       0
    question2       0
    is_duplicate    0
    dtype: int64
    ```

**3.  Tokenizer**

-   Multi30 Dataset

    ```python
        def tokenize_de(text):
            """
            Tokenizes German text from a string into a list of strings (tokens) and reverses it
            """
            return [tok.text for tok in spacy_de.tokenizer(text)][::-1]

        def tokenize_en(text):
            """
            Tokenizes English text from a string into a list of strings (tokens)
            """
            return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

-   Wikipedia Question/Answer Pairs

    ```python
        def tokenize_en(text):
            """
            Tokenizes English text from a string into a list of strings (tokens)
            """
            return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

-   Quora Dataset Release: Question Pairs
    ```python
    def tokenize_en(text):
        """
        Tokenizes English text from a string into a list of strings (tokens)
        """
        return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

**4.  Field**

-   Multi30 Dataset

    ```python

    SRC = Field(tokenize = tokenize_de, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    TRG = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)
    ```

-   Wikipedia Question/Answer Pairs

    ```python
    question = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    answer = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    fields = [('questions', question),('answers',answer)]
    ```
-   Quora Dataset Release: Question Pairs
    
    ```python
    question1 = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    question2 = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    fields = [('question1', question1),('question2',question2)]
    ```
**5.  Train and Test Dataset**

-   Multi30 Dataset

    ```python
    train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), 
                                                        fields = (SRC, TRG))

    print(f"Number of training examples: {len(train_data.examples)}")
    print(f"Number of validation examples: {len(valid_data.examples)}")
    print(f"Number of testing examples: {len(test_data.examples)}")
    Number of training examples: 29000
    Number of validation examples: 1014
    Number of testing examples: 1000
    ```

-   Wikipedia Question/Answer Pairs

    ```python
    QA_dataset_examples = [data.Example.fromlist([str(QA_dataset_complete.Question[i]),str(QA_dataset_complete.Answer[i])],fields) for i in range(QA_dataset_complete.shape[0])]
    QA_Dataset= data.Dataset(QA_dataset_examples, fields)
    (train_data, test_data) = QA_Dataset.split(split_ratio=[0.7,0.3], random_state=random.seed(SEED))

    print(f"Toatal examples: {len(QA_Dataset.examples)}")
    print(f"Number of training examples: {len(train_data.examples)}")
    # print(f"Number of validation examples: {len(valid_data.examples)}")
    print(f"Number of testing examples: {len(test_data.examples)}")

    Toatal examples: 3422
    Number of training examples: 2395
    Number of testing examples: 1027
    ```
-   Quora Dataset Release: Question Pairs
    
    ```python
        quora_duplicate_ques_examples = [data.Example.fromlist([str(quora_duplicate_ques_dataset.question1[i]),str(quora_duplicate_ques_dataset.question2[i])],fields) for i in           range(quora_duplicate_ques_dataset.shape[0])]
        quora_duplicate_dataset= data.Dataset(quora_duplicate_ques_examples, fields)
        (train_data, test_data) = quora_duplicate_dataset.split(split_ratio=[0.7,0.3], random_state=random.seed(SEED))

        print(f"Toatal examples: {len(quora_duplicate_ques_examples)}")
        print(f"Number of training examples: {len(train_data.examples)}")
        # print(f"Number of validation examples: {len(valid_data.examples)}")
        print(f"Number of testing examples: {len(test_data.examples)}")

        Toatal examples: 149263
        Number of training examples: 104484
        Number of testing examples: 44779
    ```
**6.  Examples**

-   Multi30 Dataset

    ```python

    print(vars(train_data.examples[0]))
    {'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}
    ```
-   Wikipedia Question/Answer Pairs

    ```python
        print(vars(train_data.examples[0]))
        {'questions': ['what', 'was', 'the', 'faraday', 'effect', 'first', 'called', '?'], 'answers': ['diamagnetism']}
        print(vars(test_data.examples[0]))
        {'questions': ['where', 'is', 'the', 'most', 'densely', 'populated', 'part', 'of', 'canada', '?'], 'answers': ['the', 'most', 'densely', 'populated', 'part', 'of', 'the', 'country', 'is', 'the', 'quebec', 'city', '-', 'windsor', 'corridor', 'along', 'the', 'great', 'lakes', 'and', 'saint', 'lawrence', 'river', 'in', 'the', 'southeast', '.']}
    ```

-   Quora Dataset Release: Question Pairs

    ```python
        print(vars(train_data.examples[0]))
        {'question1': ['what', 'are', 'the', 'most', 'intellectually', 'stimulating', 'movies', 'you', 'have', 'ever', 'seen', '?'], 'question2': ['what', 'are', 'the', 'most', 'intellectually', 'stimulating', 'films', 'you', 'have', 'ever', 'watched', '?']}
        print(vars(test_data.examples[0]))

        {'question1': ['can', 'my', 'girlfriend', 'track', 'my', 'phone', 'through', 'google', 'if', 'we', 'have', 'same', 'google', 'account', '?'], 'question2': ['can', 'my', 'girl', 'friend', 'track', 'my', 'phone', 'and', 'read', 'my', 'text', 'messages', 'if', 'we', 'have', 'the', 'same', 'google', 'acount']}
    ```

## Logs

-   Multi30 Dataset

    ```
    Epoch: 01 | Time: 0m 37s
        Train Loss: 5.050 | Train PPL: 156.032
         Val. Loss: 5.009 |  Val. PPL: 149.738
    Epoch: 02 | Time: 0m 38s
        Train Loss: 4.500 | Train PPL:  90.013
         Val. Loss: 4.871 |  Val. PPL: 130.418
    Epoch: 03 | Time: 0m 38s
        Train Loss: 4.205 | Train PPL:  67.024
         Val. Loss: 4.600 |  Val. PPL:  99.458
    Epoch: 04 | Time: 0m 38s
        Train Loss: 4.004 | Train PPL:  54.837
         Val. Loss: 4.545 |  Val. PPL:  94.184
    Epoch: 05 | Time: 0m 37s
        Train Loss: 3.850 | Train PPL:  46.975
         Val. Loss: 4.425 |  Val. PPL:  83.531
    Epoch: 06 | Time: 0m 38s
        Train Loss: 3.727 | Train PPL:  41.538
         Val. Loss: 4.341 |  Val. PPL:  76.754
    Epoch: 07 | Time: 0m 37s
        Train Loss: 3.599 | Train PPL:  36.571
         Val. Loss: 4.235 |  Val. PPL:  69.068
    Epoch: 08 | Time: 0m 37s
        Train Loss: 3.452 | Train PPL:  31.573
         Val. Loss: 4.115 |  Val. PPL:  61.281
    Epoch: 09 | Time: 0m 38s
        Train Loss: 3.310 | Train PPL:  27.385
         Val. Loss: 4.026 |  Val. PPL:  56.035
    Epoch: 10 | Time: 0m 38s
        Train Loss: 3.217 | Train PPL:  24.960
         Val. Loss: 3.974 |  Val. PPL:  53.217
    ```

-   Wikipedia Question/Answer Pairs
    ```
     Epoch: 01 | Time: 0m 2s
        Train Loss: 5.183 | Train PPL: 178.164
         Val. Loss: 3.600 |  Val. PPL:  36.602
    Epoch: 02 | Time: 0m 2s
        Train Loss: 4.406 | Train PPL:  81.964
         Val. Loss: 3.614 |  Val. PPL:  37.101
    Epoch: 03 | Time: 0m 2s
        Train Loss: 4.324 | Train PPL:  75.460
         Val. Loss: 3.514 |  Val. PPL:  33.595
    Epoch: 04 | Time: 0m 2s
        Train Loss: 4.246 | Train PPL:  69.828
         Val. Loss: 3.491 |  Val. PPL:  32.818
    Epoch: 05 | Time: 0m 2s
        Train Loss: 4.182 | Train PPL:  65.481
         Val. Loss: 3.457 |  Val. PPL:  31.710
    Epoch: 06 | Time: 0m 2s
        Train Loss: 4.081 | Train PPL:  59.220
         Val. Loss: 3.452 |  Val. PPL:  31.548
    Epoch: 07 | Time: 0m 2s
        Train Loss: 4.011 | Train PPL:  55.197
         Val. Loss: 3.649 |  Val. PPL:  38.448
    Epoch: 08 | Time: 0m 2s
        Train Loss: 3.959 | Train PPL:  52.417
         Val. Loss: 3.366 |  Val. PPL:  28.961
    Epoch: 09 | Time: 0m 2s
        Train Loss: 3.925 | Train PPL:  50.656
         Val. Loss: 3.393 |  Val. PPL:  29.755
    Epoch: 10 | Time: 0m 2s
        Train Loss: 3.881 | Train PPL:  48.491
         Val. Loss: 3.409 |  Val. PPL:  30.246
    ```
    
-   Quora Dataset Release: Question Pairs
    ```
    Epoch: 01 | Time: 4m 16s
        Train Loss: 4.820 | Train PPL: 123.977
         Val. Loss: 4.673 |  Val. PPL: 106.980
    Epoch: 02 | Time: 4m 17s
        Train Loss: 3.830 | Train PPL:  46.043
         Val. Loss: 4.163 |  Val. PPL:  64.232
    Epoch: 03 | Time: 4m 17s
        Train Loss: 3.293 | Train PPL:  26.912
         Val. Loss: 3.874 |  Val. PPL:  48.157
    Epoch: 04 | Time: 4m 18s
        Train Loss: 2.970 | Train PPL:  19.499
         Val. Loss: 3.765 |  Val. PPL:  43.184
    Epoch: 05 | Time: 4m 20s
        Train Loss: 2.726 | Train PPL:  15.278
         Val. Loss: 3.664 |  Val. PPL:  39.021
    Epoch: 06 | Time: 4m 19s
        Train Loss: 2.542 | Train PPL:  12.705
         Val. Loss: 3.625 |  Val. PPL:  37.537
    Epoch: 07 | Time: 4m 17s
        Train Loss: 2.405 | Train PPL:  11.076
         Val. Loss: 3.620 |  Val. PPL:  37.348
    Epoch: 08 | Time: 4m 16s
        Train Loss: 2.298 | Train PPL:   9.957
         Val. Loss: 3.566 |  Val. PPL:  35.364
    Epoch: 09 | Time: 4m 17s
        Train Loss: 2.193 | Train PPL:   8.963
         Val. Loss: 3.570 |  Val. PPL:  35.524
    Epoch: 10 | Time: 4m 16s
        Train Loss: 2.118 | Train PPL:   8.318
         Val. Loss: 3.574 |  Val. PPL:  35.665
    ```
