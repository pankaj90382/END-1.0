# Session 7 A-2 - Encoder Decoder LSTM Architecture on Multiple Data Sets

## Objective

1.  Train model with the help of Multi30K Dataset and additional Dataset from the following Links to train the Data seperately.
    1.  http://www.cs.cmu.edu/~ark/QA-data/
    2.  https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs

2.  Once done, please upload the file to GitHub and proceed to answer these questions in the S7 - Assignment Solutions, where these questions are asked:
    1.  Share the link to your GitHub repo.
    2.  Share the link to your readme file.
    3.  Copy-paste the code related to your dataset preparation for both datasets.

## Solution

**DataSets**|**Github**|**Colab**
:-----:|:-----:|:-----:
German to English Machine Translation|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/Class\_Code\_END2%20Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/Class\_Code\_END2%20Seq2seq%20Class%20Code.ipynb)
[Wikipedia question/answer pairs](http://www.cs.cmu.edu/~ark/QA-data/) from Carnegie Mellon University|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/QA\_Dataset\_Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/QA\_Dataset\_Seq2seq%20Class%20Code.ipynb)
[First Quora Dataset Release: Question Pairs](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)|[Github Link](https://github.com/pankaj90382/END-1.0/blob/main/S7/A-2/Quora\_Dataset\_Seq2seq%20Class%20Code.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S7/A-2/Quora\_Dataset\_Seq2seq%20Class%20Code.ipynb)

### Datasets

#### Wikipedia Question/Answer Pairs
The Question/Answer dataset generated by students who took undergraduate natural language processing courses taught by Noah Smith 
at Carnegie Mellon and Rebecca Hwa at the University of Pittsburgh during 
Spring 2008, Spring 2009, and Spring 2010.

There are three directories, one for each year of students: S08, S09, and S10 and each of the folder have question_answer_pairs.txt file

These file contains the totaly 6 columns out of which question and answer are our point of interset

Also note that, there are frequently multiple lines with the same question, which appear if those questions were answered 
by multiple individuals. So duplicate questions should be retained.

#### Multi30k

Multi30K dataset to stimulate multilingual multimodal research. The translations were collected from professional English-German translators contracted via an established Language Service in Germany. Translated description per image, resulting in a total of 31,014 translations. To ensure an even distribution over description length, the English descriptions were chosen based on their relative length, with an equal number of longest, shortest, and median length source descriptions.

#### Quora Dataset Release: Question Pairs
An important product principle for Quora is that there should be a single question page for each logically distinct question. As a simple example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.

The data released by quora conatins set of questions with alternative ways of asking the same questions along with tag which says wheather question is a duplicated question or not

Data set contains 6 columns out of which question1,question2 and is_duplicate fields are our interset. Only rows with is_duplicate =1 is considered

### Approach

**1.  Download and Preprocess**

-   Multi30 Dataset

    Direct from torchtext

    ```python
    from torchtext.legacy.datasets import Multi30k
    ```     
 -   Wikipedia Question/Answer Pairs

        ```python
        !wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz

        ! tar -xf /content/Question_Answer_Dataset_v1.2.tar.gz

        import glob

        import pandas as pd


        QA_dataset_complete = pd.DataFrame()

        # get all .txt files from all subdirectories

        all_files = glob.glob('./Question_Answer_Dataset_v1.2/*/*.txt')


        for file in all_files:

          QA_dataset = pd.read_csv(file, sep='\t', encoding= 'ISO-8859-1')
                                   #'unicode_escape',header=None,error_bad_lines=False,skiprows=[0])
          print(f'Complete file path :{file} | Number of records or dimentions : {QA_dataset.shape}')
          QA_dataset_complete=pd.concat([QA_dataset_complete,QA_dataset])


        print(f'After combining all files total number of records or dimentions : {QA_dataset_complete.shape}')
        ```  
-   Quora Dataset Release: Question Pairs

    ```python

    !wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv

    import pandas as pd

    quora_duplicate_ques_dataset = pd.read_csv("/content/quora_duplicate_questions.tsv", sep='\t')

    print(f' Total number of records or dimentions : {quora_duplicate_ques_dataset.shape}')

    quora_duplicate_ques_dataset = quora_duplicate_ques_dataset[quora_duplicate_ques_dataset['is_duplicate']==1]

    quora_duplicate_ques_dataset.reset_index(drop=True,  inplace=True)
    ``` 
    
**2.  Data Cleanning**

-   Multi30 Dataset

            There is no data cleanning required in Multi30k Dataset

-   Wikipedia Question/Answer Pairs

    ```python

    QA_dataset_complete.isnull().sum()
    ArticleTitle                  0
    Question                     37
    Answer                      576
    DifficultyFromQuestioner    955
    DifficultyFromAnswerer      580
    ArticleFile                   2
    dtype: int64

    QA_dataset_complete.dropna(subset=['Question','Answer'], inplace=True)
    QA_dataset_complete.shape
    (3422, 6)

    QA_dataset_complete.isnull().sum()

    ArticleTitle                  0
    Question                      0
    Answer                        0
    DifficultyFromQuestioner    688
    DifficultyFromAnswerer        5
    ArticleFile                   2
    dtype: int64
    ```

-   Quora Dataset Release: Question Pairs

    ```python
    quora_duplicate_ques_dataset.isnull().sum()

    id              0
    qid1            0
    qid2            0
    question1       0
    question2       0
    is_duplicate    0
    dtype: int64
    ```

**3.  Tokenizer**

-   Multi30 Dataset

    ```python
        def tokenize_de(text):
            """
            Tokenizes German text from a string into a list of strings (tokens) and reverses it
            """
            return [tok.text for tok in spacy_de.tokenizer(text)][::-1]

        def tokenize_en(text):
            """
            Tokenizes English text from a string into a list of strings (tokens)
            """
            return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

-   Wikipedia Question/Answer Pairs

    ```python
        def tokenize_en(text):
            """
            Tokenizes English text from a string into a list of strings (tokens)
            """
            return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

-   Quora Dataset Release: Question Pairs
    ```python
    def tokenize_en(text):
        """
        Tokenizes English text from a string into a list of strings (tokens)
        """
        return [tok.text for tok in spacy_en.tokenizer(text)]
    ```

**4.  Field**

-   Multi30 Dataset

    ```python

    SRC = Field(tokenize = tokenize_de, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    TRG = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)
    ```

-   Wikipedia Question/Answer Pairs

    ```python
    question = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    answer = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    fields = [('questions', question),('answers',answer)]
    ```
-   Quora Dataset Release: Question Pairs
    
    ```python
    question1 = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    question2 = Field(tokenize = tokenize_en, 
                init_token = '<sos>', 
                eos_token = '<eos>', 
                lower = True)

    fields = [('question1', question1),('question2',question2)]
    ```
**5.  Train and Test Dataset**

-   Multi30 Dataset

    ```python
    train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), 
                                                        fields = (SRC, TRG))

    print(f"Number of training examples: {len(train_data.examples)}")
    print(f"Number of validation examples: {len(valid_data.examples)}")
    print(f"Number of testing examples: {len(test_data.examples)}")
    Number of training examples: 29000
    Number of validation examples: 1014
    Number of testing examples: 1000
    ```

-   Wikipedia Question/Answer Pairs

    ```python
    QA_dataset_examples = [data.Example.fromlist([str(QA_dataset_complete.Question[i]),str(QA_dataset_complete.Answer[i])],fields) for i in range(QA_dataset_complete.shape[0])]
    QA_Dataset= data.Dataset(QA_dataset_examples, fields)
    (train_data, test_data) = QA_Dataset.split(split_ratio=[0.7,0.3], random_state=random.seed(SEED))

    print(f"Toatal examples: {len(QA_Dataset.examples)}")
    print(f"Number of training examples: {len(train_data.examples)}")
    # print(f"Number of validation examples: {len(valid_data.examples)}")
    print(f"Number of testing examples: {len(test_data.examples)}")

    Toatal examples: 3422
    Number of training examples: 2395
    Number of testing examples: 1027
    ```
-   Quora Dataset Release: Question Pairs
    
    ```python
        quora_duplicate_ques_examples = [data.Example.fromlist([str(quora_duplicate_ques_dataset.question1[i]),str(quora_duplicate_ques_dataset.question2[i])],fields) for i in           range(quora_duplicate_ques_dataset.shape[0])]
        quora_duplicate_dataset= data.Dataset(quora_duplicate_ques_examples, fields)
        (train_data, test_data) = quora_duplicate_dataset.split(split_ratio=[0.7,0.3], random_state=random.seed(SEED))

        print(f"Toatal examples: {len(quora_duplicate_ques_examples)}")
        print(f"Number of training examples: {len(train_data.examples)}")
        # print(f"Number of validation examples: {len(valid_data.examples)}")
        print(f"Number of testing examples: {len(test_data.examples)}")

        Toatal examples: 149263
        Number of training examples: 104484
        Number of testing examples: 44779
    ```
  
 
