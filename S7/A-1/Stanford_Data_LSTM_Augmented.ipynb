{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Stanford_Data_LSTM_Augmented.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHutK7N+1Ahpf1lhKR9nCd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tAE_DRf7fRFi"},"source":["## Downloading Stanford Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9gLQ3FoE4n3","executionInfo":{"status":"ok","timestamp":1623957560641,"user_tz":-330,"elapsed":619,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"0d41577c-18a5-4dea-eabc-c3c064324149"},"source":["!nvidia-smi "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Jun 17 19:19:20 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7fuUIuqe6Pk","executionInfo":{"status":"ok","timestamp":1623957561316,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# !wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_B4gO-QrfP7q","executionInfo":{"status":"ok","timestamp":1623957561316,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# !unzip /content/stanfordSentimentTreebank.zip"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4b_ol6RDZF7-"},"source":["## Processing Stanford Sentimental Data"]},{"cell_type":"code","metadata":{"id":"mDNpHBt5fcDO","executionInfo":{"status":"ok","timestamp":1623957561317,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import os\n","import sys\n","\n","import pandas\n","def clean_data(x):\n","  char_dict = {\n","          '-LRB-' : '(',\n","          '-RRB-' : ')',\n","          '\\xa0' : ' ',\n","          '\\xc2' : '',\n","          '\\xc3\\x83\\xc2\\xa0' : 'a',\n","          'à' : 'a',\n","          'Â' : '',\n","          'â' : 'a',\n","          'ã' : 'a',\n","          'Ã¡' : 'a',\n","          'Ã¢' : 'a',\n","          'Ã£' : 'a',\n","          'Ã¦' : 'ae',\n","          'Ã§' : 'c',\n","          'Ã¨' : 'e',\n","          'Ã©' : 'e',\n","          'Ã­' : 'i',\n","          'Ã¯' : 'i',\n","          'Ã±' : 'n',\n","          'Ã³' : 'o',\n","          'Ã´' : 'o',\n","          'Ã¶' : 'o',\n","          'Ã»' : 'u',\n","          'Ã¼' : 'u',\n","          'æ' : 'ae',\n","          'ç' : 'c',\n","          'è' : 'e',\n","          'é' : 'e',\n","          'í' : 'i',\n","          'ï' : 'i',\n","          'ñ' : 'n',\n","          'ó' : 'o',\n","          'ô' : 'o',\n","          'ö' : 'o',\n","          'û' : 'u',\n","          'ü' : 'u'\n","      }\n","  for keys in char_dict.keys():\n","    x = x.replace(keys, char_dict[keys])\n","  return x\n","\n","def get_phrase_sentiments(base_directory):\n","    def group_labels(label):\n","        if label in [\"very negative\", \"negative\"]:\n","            return \"negative\"\n","        elif label in [\"positive\", \"very positive\"]:\n","            return \"positive\"\n","        else:\n","            return \"neutral\"\n","\n","    dictionary = pandas.read_csv(os.path.join(base_directory, \"dictionary.txt\"), sep=\"|\")\n","    dictionary.columns = [\"phrase\", \"id\"]\n","    dictionary = dictionary.set_index(\"id\")\n","\n","    sentiment_labels = pandas.read_csv(os.path.join(base_directory, \"sentiment_labels.txt\"), sep=\"|\")\n","    sentiment_labels.columns = [\"id\", \"sentiment\"]\n","    sentiment_labels = sentiment_labels.set_index(\"id\")\n","\n","    phrase_sentiments = dictionary.join(sentiment_labels)\n","\n","    phrase_sentiments[\"fine\"] = pandas.cut(phrase_sentiments.sentiment, [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n","                                           include_lowest=True,\n","                                           labels=[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"])\n","    phrase_sentiments[\"coarse\"] = phrase_sentiments.fine.apply(group_labels)\n","    return phrase_sentiments\n","\n","\n","def get_sentence_partitions(base_directory):\n","    sentences = pandas.read_csv(os.path.join(base_directory, \"datasetSentences.txt\"), index_col=\"sentence_index\",\n","                                sep=\"\\t\")\n","    splits = pandas.read_csv(os.path.join(base_directory, \"datasetSplit.txt\"), index_col=\"sentence_index\")\n","    return sentences.join(splits)\n","\n","\n","def partition(base_directory):\n","    phrase_sentiments = get_phrase_sentiments(base_directory).reset_index(level=0)\n","    sentence_partitions = get_sentence_partitions(base_directory)\n","    # noinspection PyUnresolvedReferences\n","    phrase_sentiments['phrase'] = phrase_sentiments['phrase'].apply(lambda x : clean_data(x))\n","    sentence_partitions['sentence'] = sentence_partitions['sentence'].apply(lambda x : clean_data(x))\n","    data = pandas.merge(sentence_partitions, phrase_sentiments, right_on=\"phrase\", left_on=\"sentence\", how='left')\n","    data[\"splitset_label\"] = data[\"splitset_label\"].fillna(1).astype(int)\n","    data[\"sentence\"] = data[\"sentence\"].str.replace(r\"\\s('s|'d|'re|'ll|'m|'ve|n't)\\b\", lambda m: m.group(1))\n","    return data.groupby(\"splitset_label\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAFlUE7bjGv4","executionInfo":{"status":"ok","timestamp":1623957563791,"user_tz":-330,"elapsed":2490,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["base_directory, output_directory = '/content/stanfordSentimentTreebank','/content/Dataset/';\n","os.makedirs(output_directory, exist_ok=True)\n","for splitset, partition in partition(base_directory):\n","    split_name = {1: \"train\", 2: \"test\", 3: \"dev\"}[splitset]\n","    filename = os.path.join(output_directory, \"stanford-sentiment-treebank.%s.csv\" % split_name)\n","    del partition[\"splitset_label\"]\n","    partition.to_csv(filename)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mB35xAvnZOTb"},"source":["## Reading the refined CSV's"]},{"cell_type":"code","metadata":{"id":"Vry0hIF5jVnM","executionInfo":{"status":"ok","timestamp":1623957563792,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import pandas as pd\n","train_data = pd.read_csv('/content/Dataset/stanford-sentiment-treebank.train.csv')\n","test_data = pd.read_csv('/content/Dataset/stanford-sentiment-treebank.test.csv')\n","dev_data = pd.read_csv('/content/Dataset/stanford-sentiment-treebank.dev.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lUNM1MdrZTWX"},"source":["## Shape of the Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQAveg9Nowmo","executionInfo":{"status":"ok","timestamp":1623957563792,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"8004345b-2ab3-4bca-cc32-7ab5fdab6084"},"source":["print(train_data.shape)\n","print(test_data.shape)\n","print(dev_data.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(8544, 7)\n","(2217, 7)\n","(1101, 7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XHuONw_lZ5VE"},"source":["## Checking the Labels"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3PoBLsoZ9o8","executionInfo":{"status":"ok","timestamp":1623957563793,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"c61a418d-6fde-4d57-eb31-18a3b70bbdce"},"source":["print(train_data.fine.value_counts())\n","print(test_data.fine.value_counts())\n","print(dev_data.fine.value_counts())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["positive         2321\n","negative         2218\n","neutral          1623\n","very positive    1287\n","very negative    1092\n","Name: fine, dtype: int64\n","negative         633\n","positive         515\n","very positive    400\n","neutral          390\n","very negative    279\n","Name: fine, dtype: int64\n","negative         289\n","positive         279\n","neutral          229\n","very positive    164\n","very negative    139\n","Name: fine, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i38ARxFBaN57"},"source":["Upscale the labels to fit into 25 classes of sentiments according to the paper. Currently the maximum Categories are 82."]},{"cell_type":"markdown","metadata":{"id":"Cf2UFrCPcrOy"},"source":["## Analysis of Null values and interpolate them."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKxyXVUzcvjn","executionInfo":{"status":"ok","timestamp":1623957563793,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"da8dee9b-b56c-470c-f94f-253eb145b561"},"source":["print(\"The Total null values in Train Data:- \",train_data['fine'].isnull().sum())\n","print(\"The Total null values in Test Data:- \",test_data['fine'].isnull().sum())\n","print(\"The Total null values in Dev Data:- \",dev_data['fine'].isnull().sum())"],"execution_count":9,"outputs":[{"output_type":"stream","text":["The Total null values in Train Data:-  3\n","The Total null values in Test Data:-  0\n","The Total null values in Dev Data:-  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CYSI83alfYJi","executionInfo":{"status":"ok","timestamp":1623957563793,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_data.dropna(subset=['fine'], inplace=True)\n","# test_data.sentiment.interpolate(method ='linear', limit_direction ='forward',inplace=True)\n","dev_data.dropna(subset=['fine'], inplace=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XUBiq1ffxAx","executionInfo":{"status":"ok","timestamp":1623957563794,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"3d3a05b0-3a44-4773-d24b-c338af407aca"},"source":["print(\"The Total null values in Train Data:- \",train_data['fine'].isnull().sum())\n","print(\"The Total null values in Test Data:- \",test_data['fine'].isnull().sum())\n","print(\"The Total null values in Dev Data:- \",dev_data['fine'].isnull().sum())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["The Total null values in Train Data:-  0\n","The Total null values in Test Data:-  0\n","The Total null values in Dev Data:-  0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"60RIYmR2pKbb"},"source":["## Appending Data"]},{"cell_type":"code","metadata":{"id":"_eABMn85Cj9Q","executionInfo":{"status":"ok","timestamp":1623957563794,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_data = train_data.append(test_data, ignore_index=True)\n","train_data = train_data.append(dev_data, ignore_index=True)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"8p7YYOWVG2X_","executionInfo":{"status":"ok","timestamp":1623957563794,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_data.to_csv('/content/Dataset/stanfored_final_data.csv')"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1IT0pCaetRqz"},"source":["## TorchText"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atCbkNIdtVA5","executionInfo":{"status":"ok","timestamp":1623957564439,"user_tz":-330,"elapsed":651,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"ea816695-7b4e-4022-e574-b3e2ebc55e64"},"source":["# Import Library\n","import random\n","import torch, torchtext\n","from torchtext.legacy import data \n","\n","# Manual Seed\n","SEED = 43\n","torch.manual_seed(SEED)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7faa0fd512b0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"bZI6OUXetdhJ","executionInfo":{"status":"ok","timestamp":1623957565912,"user_tz":-330,"elapsed":1475,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n","Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6JNS7FgtxRa","executionInfo":{"status":"ok","timestamp":1623957565915,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["fields = [('sentence', Sentence),('label',Label)]"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"-2-HS7FN0Rsh","executionInfo":{"status":"ok","timestamp":1623957568079,"user_tz":-330,"elapsed":2169,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_example = [data.Example.fromlist([train_data.sentence[i],train_data.fine[i]], fields) for i in range(train_data.shape[0])]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcM6tLeM0ZAj","executionInfo":{"status":"ok","timestamp":1623957568085,"user_tz":-330,"elapsed":43,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# Creating dataset\n","\n","train_dataset = data.Dataset(train_example, fields)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgT7YkszRjVP","executionInfo":{"status":"ok","timestamp":1623957568086,"user_tz":-330,"elapsed":41,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"70a363de-0d0d-4dc0-e354-5965c409bd23"},"source":["vars(train_dataset.examples[10])"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 'very positive',\n"," 'sentence': ['Good',\n","  'fun',\n","  ',',\n","  'good',\n","  'action',\n","  ',',\n","  'good',\n","  'acting',\n","  ',',\n","  'good',\n","  'dialogue',\n","  ',',\n","  'good',\n","  'pace',\n","  ',',\n","  'good',\n","  'cinematography',\n","  '.']}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"x2rHb52A9UVS","executionInfo":{"status":"ok","timestamp":1623957568086,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["(train, test) = train_dataset.split(split_ratio=[80, 20], random_state = random.seed(SEED))"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcWMV-CP9sG6","executionInfo":{"status":"ok","timestamp":1623957568086,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"d02453c8-435f-43b0-cdcb-39a07e821135"},"source":["len(train), len(test)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(9486, 2372)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"8jLRDgfSRpjn","executionInfo":{"status":"ok","timestamp":1623957568087,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# MAX_VOCAB_SIZE = 25000\n","Sentence.build_vocab(train)\n","Label.build_vocab(train)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCF1-v1KRy7g","executionInfo":{"status":"ok","timestamp":1623957568087,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"9dff907b-bef4-4938-b58c-297aafdb14e6"},"source":["print('Size of input vocab : ', len(Sentence.vocab))\n","print('Size of label vocab : ', len(Label.vocab))\n","print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n","print('Labels : ', Label.vocab.stoi)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Size of input vocab :  18173\n","Size of label vocab :  5\n","Top 10 words appreared repeatedly : [('.', 8951), (',', 8010), ('the', 6734), ('and', 4976), ('a', 4895), ('of', 4891), ('to', 3421), ('-', 3088), (\"'s\", 2881), ('is', 2867)]\n","Labels :  defaultdict(None, {'negative': 0, 'positive': 1, 'neutral': 2, 'very positive': 3, 'very negative': 4})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lMiKkP2DR-Tx","executionInfo":{"status":"ok","timestamp":1623957568087,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXipHWHSR_ax","executionInfo":{"status":"ok","timestamp":1623957568087,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_iterator, test_iterator = data.BucketIterator.splits((train, test), batch_size = 32, \n","                                                            sort_key = lambda x: len(x.sentence),\n","                                                            sort_within_batch=True, device = device)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-NEy5yZSNxw","executionInfo":{"status":"ok","timestamp":1623957568088,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import os, pickle\n","with open('tokenizer.pkl', 'wb') as tokens: \n","    pickle.dump({'input_stoi':Sentence.vocab.stoi, 'label_itos':Label.vocab.itos}, tokens)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtf2y-pfSYDq","executionInfo":{"status":"ok","timestamp":1623957568088,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class classifier(nn.Module):\n","    \n","    # Define all the layers used in model\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n","        \n","        super().__init__()          \n","        \n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        # LSTM layer\n","        self.encoder = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           dropout=dropout,\n","                           batch_first=True,\n","                           bidirectional=True)\n","        # try using nn.GRU or nn.RNN here and compare their performances\n","        # try bidirectional and compare their performances\n","        self.projection = nn.Sequential(nn.Linear(2 * hidden_dim,hidden_dim),nn.BatchNorm1d(hidden_dim),nn.ReLU(), nn.Dropout(dropout)) \n","        # Dense layer\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        # text = [batch size, sent_length]\n","        embedded = self.embedding(text)\n","        # embedded = [batch size, sent_len, emb dim]\n","      \n","        # packed sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n","        \n","        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n","        #hidden = [batch size, num layers * num directions,hid dim]\n","        #cell = [batch size, num layers * num directions,hid dim]\n","\n","        projection = self.projection(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","        # Hidden = [batch size, hid dim * num directions]\n","        dense_outputs = self.fc(projection)   \n","        \n","        # Final activation function softmax\n","        output = F.softmax(dense_outputs, dim=1)\n","            \n","        return output"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQIC2Z1WSi2x","executionInfo":{"status":"ok","timestamp":1623957568088,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# Define hyperparameters\n","size_of_vocab = len(Sentence.vocab)\n","embedding_dim = 100\n","num_hidden_nodes = 100\n","num_output_nodes = len(Label.vocab)\n","num_layers = 2\n","dropout = 0.5\n","PAD_IDX = Sentence.vocab.stoi[Sentence.pad_token]\n","\n","# Instantiate the model\n","model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout, PAD_IDX)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxggrPxUSp9_","executionInfo":{"status":"ok","timestamp":1623957568089,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"8abda81d-f0fb-41ab-8456-04a82f8284f9"},"source":["print(model)\n","\n","#No. of trianable parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["classifier(\n","  (embedding): Embedding(18173, 100, padding_idx=1)\n","  (encoder): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n","  (projection): Sequential(\n","    (0): Linear(in_features=200, out_features=100, bias=True)\n","    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.5, inplace=False)\n","  )\n","  (fc): Linear(in_features=100, out_features=5, bias=True)\n",")\n","The model has 2,241,305 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ay-IDVbwSsnX","executionInfo":{"status":"ok","timestamp":1623957571476,"user_tz":-330,"elapsed":3398,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import torch.optim as optim\n","\n","# define optimizer and loss\n","optimizer = optim.Adam(model.parameters(), lr=5e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","# define metric\n","def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    _, predictions = torch.max(preds, 1)\n","    correct = (predictions == y).float()\n","    acc = correct.sum() / len(correct)\n","    return acc\n","    \n","# push to cuda if available\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3WiFxGESvNM","executionInfo":{"status":"ok","timestamp":1623957571476,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    # initialize every epoch \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # set the model in training phase\n","    model.train()  \n","    \n","    for batch in iterator:\n","        \n","        # resets the gradients after every batch\n","        optimizer.zero_grad()   \n","        \n","        # retrieve text and no. of words\n","        sentence, sentence_lengths = batch.sentence   \n","        \n","        # convert to 1D tensor\n","        predictions = model(sentence, sentence_lengths).squeeze()  \n","        \n","        # print(predictions)\n","        # compute the loss\n","        loss = criterion(predictions, batch.label)        \n","        \n","        # compute the binary accuracy\n","        acc = binary_accuracy(predictions, batch.label)   \n","        \n","        # backpropage the loss and compute the gradients\n","        loss.backward()       \n","        \n","        # update the weights\n","        optimizer.step()      \n","        \n","        # loss and accuracy\n","        epoch_loss += loss.item()  \n","        epoch_acc += acc.item()    \n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGpZATCmSyOw","executionInfo":{"status":"ok","timestamp":1623957571477,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    # initialize every epoch\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    # deactivating dropout layers\n","    model.eval()\n","    \n","    # deactivates autograd\n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","        \n","            # retrieve text and no. of words\n","            sentence, sentence_lengths = batch.sentence\n","            \n","            # convert to 1d tensor\n","            predictions = model(sentence, sentence_lengths).squeeze()\n","            \n","            # compute loss and accuracy\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            \n","            # keep track of loss and accuracy\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MNQNdyDS157","executionInfo":{"status":"ok","timestamp":1623957619094,"user_tz":-330,"elapsed":47621,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"11e2a55e-703f-4c8d-bba7-0506a9b9e7f6"},"source":["N_EPOCHS = 20\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","     \n","    # train the model\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    \n","    # evaluate the model\n","    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","    # dev_loss, dev_acc = evaluate(model, dev_iterator, criterion)\n","    \n","    # save the best model\n","    if test_loss < best_valid_loss:\n","        best_valid_loss = test_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}% \\n')\n","    # print(f'\\t Dev Loss: {dev_loss:.3f} |  Dev Acc: {dev_acc*100:.2f}% \\n')"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\tTrain Loss: 1.579 | Train Acc: 28.53%\n","\t Test Loss: 1.561 |  Test Acc: 31.62% \n","\n","\tTrain Loss: 1.543 | Train Acc: 34.01%\n","\t Test Loss: 1.545 |  Test Acc: 33.38% \n","\n","\tTrain Loss: 1.502 | Train Acc: 38.36%\n","\t Test Loss: 1.540 |  Test Acc: 33.92% \n","\n","\tTrain Loss: 1.463 | Train Acc: 42.87%\n","\t Test Loss: 1.525 |  Test Acc: 36.04% \n","\n","\tTrain Loss: 1.420 | Train Acc: 48.17%\n","\t Test Loss: 1.528 |  Test Acc: 34.92% \n","\n","\tTrain Loss: 1.375 | Train Acc: 52.56%\n","\t Test Loss: 1.530 |  Test Acc: 35.00% \n","\n","\tTrain Loss: 1.341 | Train Acc: 56.16%\n","\t Test Loss: 1.538 |  Test Acc: 34.46% \n","\n","\tTrain Loss: 1.303 | Train Acc: 60.01%\n","\t Test Loss: 1.527 |  Test Acc: 35.58% \n","\n","\tTrain Loss: 1.277 | Train Acc: 62.69%\n","\t Test Loss: 1.529 |  Test Acc: 35.75% \n","\n","\tTrain Loss: 1.257 | Train Acc: 64.65%\n","\t Test Loss: 1.536 |  Test Acc: 34.83% \n","\n","\tTrain Loss: 1.233 | Train Acc: 66.84%\n","\t Test Loss: 1.531 |  Test Acc: 35.83% \n","\n","\tTrain Loss: 1.216 | Train Acc: 68.75%\n","\t Test Loss: 1.541 |  Test Acc: 34.71% \n","\n","\tTrain Loss: 1.207 | Train Acc: 69.44%\n","\t Test Loss: 1.549 |  Test Acc: 33.88% \n","\n","\tTrain Loss: 1.189 | Train Acc: 71.31%\n","\t Test Loss: 1.540 |  Test Acc: 35.00% \n","\n","\tTrain Loss: 1.176 | Train Acc: 72.81%\n","\t Test Loss: 1.554 |  Test Acc: 33.46% \n","\n","\tTrain Loss: 1.170 | Train Acc: 73.15%\n","\t Test Loss: 1.555 |  Test Acc: 33.38% \n","\n","\tTrain Loss: 1.157 | Train Acc: 74.73%\n","\t Test Loss: 1.550 |  Test Acc: 33.96% \n","\n","\tTrain Loss: 1.141 | Train Acc: 76.57%\n","\t Test Loss: 1.545 |  Test Acc: 34.29% \n","\n","\tTrain Loss: 1.131 | Train Acc: 77.51%\n","\t Test Loss: 1.558 |  Test Acc: 32.83% \n","\n","\tTrain Loss: 1.115 | Train Acc: 79.24%\n","\t Test Loss: 1.561 |  Test Acc: 32.79% \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qXn3htKVVil9","executionInfo":{"status":"ok","timestamp":1623957619095,"user_tz":-330,"elapsed":43,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# torch.save(model.state_dict(), 'last_saved_weights.pt')"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xUO6FLNlsyv","executionInfo":{"status":"ok","timestamp":1623957619732,"user_tz":-330,"elapsed":677,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["path='./saved_weights.pt'\n","model.load_state_dict(torch.load(path));\n","model.eval();\n","with open('./tokenizer.pkl', 'rb') as f:\n","  meta_data = pickle.load(f)\n","tokenizer = meta_data['input_stoi']\n","categories = meta_data['label_itos']\n","\n","#inference \n","\n","import spacy\n","nlp = spacy.load('en')\n","\n","def classify_tweet(tweet):\n","    \n","    #categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n","    \n","    # tokenize the tweet \n","    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n","    # convert to integer sequence using predefined tokenizer dictionary\n","    indexed = [tokenizer[t] for t in tokenized]        \n","    # compute no. of words        \n","    length = [len(indexed)]\n","    # convert to tensor                                    \n","    tensor = torch.LongTensor(indexed).to(device)   \n","    # reshape in form of batch, no. of words           \n","    tensor = tensor.unsqueeze(1).T  \n","    # convert to tensor                          \n","    length_tensor = torch.LongTensor(length)\n","    # Get the model prediction                  \n","    prediction = model(tensor, length_tensor)\n","\n","    _, pred = torch.max(prediction, 1) \n","    \n","    return categories[pred.item()]\n","    # return pred.item()"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caLI-r6Km0jr"},"source":["## Setimental Analysis of Train, Test and Dev Data"]},{"cell_type":"code","metadata":{"id":"5dvH7uGdmFsn","executionInfo":{"status":"ok","timestamp":1623957643146,"user_tz":-330,"elapsed":23419,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["train_data['Predicted_Label'] = train_data['sentence'].apply(classify_tweet)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMzzvGBim_ak","executionInfo":{"status":"ok","timestamp":1623957643147,"user_tz":-330,"elapsed":49,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# test_data['Predicted_Label'] = test_data['sentence'].apply(classify_tweet)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VpBZMA3nC_c","executionInfo":{"status":"ok","timestamp":1623957643149,"user_tz":-330,"elapsed":47,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# dev_data['Predicted_Label'] = test_data['sentence'].apply(classify_tweet)"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLgXEMBsmGdE"},"source":["## Postive Tweets"]},{"cell_type":"code","metadata":{"id":"sP6qfCwjnaFN","executionInfo":{"status":"ok","timestamp":1623957643149,"user_tz":-330,"elapsed":44,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["import numpy as np\n","train_data['Flag'] = np.where(train_data['Predicted_Label']==train_data['fine'], 1, 0)\n","# test_data['Flag'] = np.where(test_data['Predicted_Label']==test_data['label'], 1, 0)\n","# dev_data['Flag'] = np.where(dev_data['Predicted_Label']==dev_data['label'], 1, 0)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bX2higbnKM-"},"source":["## Train Postive Tweets"]},{"cell_type":"code","metadata":{"id":"5cSP9U6anHMI","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1623957643150,"user_tz":-330,"elapsed":42,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"fc581213-c982-4a25-84aa-d1b672426a09"},"source":["train_data[train_data['Flag']==1]"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentence</th>\n","      <th>id</th>\n","      <th>phrase</th>\n","      <th>sentiment</th>\n","      <th>fine</th>\n","      <th>coarse</th>\n","      <th>Predicted_Label</th>\n","      <th>Flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>63</td>\n","      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n","      <td>225801.0</td>\n","      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n","      <td>0.62500</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>64</td>\n","      <td>You'd think by now America would have had enou...</td>\n","      <td>14646.0</td>\n","      <td>You 'd think by now America would have had eno...</td>\n","      <td>0.50000</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>65</td>\n","      <td>Yet the act is still charming here .</td>\n","      <td>14644.0</td>\n","      <td>Yet the act is still charming here .</td>\n","      <td>0.72222</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>74</td>\n","      <td>Part of the charm of Satin Rouge is that it av...</td>\n","      <td>225402.0</td>\n","      <td>Part of the charm of Satin Rouge is that it av...</td>\n","      <td>0.72222</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>138</td>\n","      <td>Still , this flick is fun , and host to some t...</td>\n","      <td>225973.0</td>\n","      <td>Still , this flick is fun , and host to some t...</td>\n","      <td>0.81944</td>\n","      <td>very positive</td>\n","      <td>positive</td>\n","      <td>very positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11848</th>\n","      <td>7901</td>\n","      <td>But it could have been worse .</td>\n","      <td>222770.0</td>\n","      <td>But it could have been worse .</td>\n","      <td>0.36111</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11849</th>\n","      <td>7902</td>\n","      <td>Some of their jokes work , but most fail miser...</td>\n","      <td>148746.0</td>\n","      <td>Some of their jokes work , but most fail miser...</td>\n","      <td>0.20833</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11852</th>\n","      <td>7905</td>\n","      <td>... Designed to provide a mix of smiles and te...</td>\n","      <td>221766.0</td>\n","      <td>... Designed to provide a mix of smiles and te...</td>\n","      <td>0.22222</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11853</th>\n","      <td>7906</td>\n","      <td>it seems to me the film is about the art of ri...</td>\n","      <td>163906.0</td>\n","      <td>it seems to me the film is about the art of ri...</td>\n","      <td>0.29167</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11857</th>\n","      <td>7910</td>\n","      <td>Schaeffer has to find some hook on which to ha...</td>\n","      <td>148419.0</td>\n","      <td>Schaeffer has to find some hook on which to ha...</td>\n","      <td>0.27778</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5479 rows × 9 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ... Flag\n","2              63  ...    1\n","3              64  ...    1\n","4              65  ...    1\n","7              74  ...    1\n","14            138  ...    1\n","...           ...  ...  ...\n","11848        7901  ...    1\n","11849        7902  ...    1\n","11852        7905  ...    1\n","11853        7906  ...    1\n","11857        7910  ...    1\n","\n","[5479 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"ROyS9hNUoqHA"},"source":["## Train Set Negative Tweets"]},{"cell_type":"code","metadata":{"id":"IpbTXpP6of7c","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"ok","timestamp":1623957643151,"user_tz":-330,"elapsed":38,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"7ebf0830-57a6-4bf9-dbb0-f7ce4e9c5187"},"source":["train_data[train_data['Flag']==0]"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentence</th>\n","      <th>id</th>\n","      <th>phrase</th>\n","      <th>sentiment</th>\n","      <th>fine</th>\n","      <th>coarse</th>\n","      <th>Predicted_Label</th>\n","      <th>Flag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>The Rock is destined to be the 21st Century's ...</td>\n","      <td>226166.0</td>\n","      <td>The Rock is destined to be the 21st Century 's...</td>\n","      <td>0.69444</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>The gorgeously elaborate continuation of `` Th...</td>\n","      <td>226300.0</td>\n","      <td>The gorgeously elaborate continuation of `` Th...</td>\n","      <td>0.83333</td>\n","      <td>very positive</td>\n","      <td>positive</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>66</td>\n","      <td>Whether or not you're enlightened by any of De...</td>\n","      <td>227114.0</td>\n","      <td>Whether or not you 're enlightened by any of D...</td>\n","      <td>0.83333</td>\n","      <td>very positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>70</td>\n","      <td>Just the labour involved in creating the layer...</td>\n","      <td>224508.0</td>\n","      <td>Just the labour involved in creating the layer...</td>\n","      <td>0.87500</td>\n","      <td>very positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>84</td>\n","      <td>a screenplay more ingeniously constructed than...</td>\n","      <td>228134.0</td>\n","      <td>a screenplay more ingeniously constructed than...</td>\n","      <td>0.83333</td>\n","      <td>very positive</td>\n","      <td>positive</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11850</th>\n","      <td>7903</td>\n","      <td>Even horror fans will most likely not find wha...</td>\n","      <td>145161.0</td>\n","      <td>Even horror fans will most likely not find wha...</td>\n","      <td>0.12500</td>\n","      <td>very negative</td>\n","      <td>negative</td>\n","      <td>positive</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11851</th>\n","      <td>7904</td>\n","      <td>comes off like a rejected ABC Afterschool Spec...</td>\n","      <td>229921.0</td>\n","      <td>comes off like a rejected ABC Afterschool Spec...</td>\n","      <td>0.16667</td>\n","      <td>very negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11854</th>\n","      <td>7907</td>\n","      <td>It's just disappointingly superficial -- a mov...</td>\n","      <td>146522.0</td>\n","      <td>It 's just disappointingly superficial -- a mo...</td>\n","      <td>0.33333</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>positive</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11855</th>\n","      <td>7908</td>\n","      <td>The title not only describes its main characte...</td>\n","      <td>149944.0</td>\n","      <td>The title not only describes its main characte...</td>\n","      <td>0.23611</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>positive</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11856</th>\n","      <td>7909</td>\n","      <td>Sometimes it feels as if it might have been ma...</td>\n","      <td>148760.0</td>\n","      <td>Sometimes it feels as if it might have been ma...</td>\n","      <td>0.44444</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6379 rows × 9 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ... Flag\n","0               0  ...    0\n","1               1  ...    0\n","5              66  ...    0\n","6              70  ...    0\n","8              84  ...    0\n","...           ...  ...  ...\n","11850        7903  ...    0\n","11851        7904  ...    0\n","11854        7907  ...    0\n","11855        7908  ...    0\n","11856        7909  ...    0\n","\n","[6379 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"Y199OtOQokb6","executionInfo":{"status":"ok","timestamp":1623957643778,"user_tz":-330,"elapsed":663,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["# dev_data.to_csv('/content/Dataset/final_dev.csv',index=False)\n","train_data.to_csv('/content/Dataset/final_train.csv',index=False)\n","# test_data.to_csv('/content/Dataset/final_test.csv',index=False)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG32q84NU9oH","executionInfo":{"status":"ok","timestamp":1623957643779,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":["!mv /content/tokenizer.pkl /content/Dataset/\n","!mv /content/saved_weights.pt /content/Dataset/"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"aR2Jnso9pRiC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623957644670,"user_tz":-330,"elapsed":899,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}},"outputId":"a11f8a21-1062-4be2-fa3a-f451d91db369"},"source":["!zip Dataset.zip /content/Dataset/*"],"execution_count":44,"outputs":[{"output_type":"stream","text":["updating: content/Dataset/final_train.csv (deflated 76%)\n","updating: content/Dataset/saved_weights.pt (deflated 8%)\n","updating: content/Dataset/stanford-sentiment-treebank.dev.csv (deflated 75%)\n","updating: content/Dataset/stanford-sentiment-treebank.test.csv (deflated 75%)\n","updating: content/Dataset/stanford-sentiment-treebank.train.csv (deflated 75%)\n","updating: content/Dataset/stanfored_final_data.csv (deflated 74%)\n","updating: content/Dataset/tokenizer.pkl (deflated 48%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lk_SZP1uYE0N","executionInfo":{"status":"ok","timestamp":1623957644674,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sachin Salman","photoUrl":"","userId":"05425883655044061385"}}},"source":[""],"execution_count":44,"outputs":[]}]}