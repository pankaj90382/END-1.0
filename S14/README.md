# Session 14 BERT AND BART (Transformers)

## Assignment

1.  TASK 1: Train BERT using the code mentioned  [here (Links to an external site.)](https://drive.google.com/file/d/1Zp2_Uka8oGDYsSe5ELk-xz6wIX8OIkB7/view?usp=sharing)  on the Squad Dataset for 20% overall samples (1/5 Epochs). Show results on 5 samples.
2.  TASK 2: Reproductive  [these (Links to an external site.)](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)  results, and show output on 5 samples.
3.  TASK 3: Reproduce the training explained in this  [blog (Links to an external site.)](https://towardsdatascience.com/bart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c). You can decide to pick fewer datasets.
4.  Proceed to Session 14 - Assignment Solutions page and:
    1.  Submit README link for Task 1 (training log snippets and 5 sample results along with BERT description must be available) - 750
    2.  Submit README link for Task 2 (training log snippets and 5 sample results) - 250
    3.  Submit README link for Task 3 (training log snippets and 5 sample results along with BART description must be available) - 1000

## Solution

   * [Task 1](#task1)
   * [Task 2](#task2)
   * [Task 3](#task3)

<a id="task1"></a>
### BERT QA Bot on SQUAD Dataset

<a id="task2"></a>
 ### BERT Sentence Classification
 
 <a id="task3"></a>
 ### BART Paraphrasing
 
 
 ## Refrences
