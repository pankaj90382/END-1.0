# Session 10 Language Translation using Seq2Seq Attention with Glove Embedding

## Objective
1. Replace the embeddings of [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) code with GloVe embeddings
2. Compare your results with [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) code. 


## Solution

### GloVe
GloVe, stands for "Global Vectors", is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.<br>

### Dataset
The data consists of a set of thousands of English to French translation pairs. Each word in both the the languages will be represented as a one-hot vector. This process is handled by the Lang class. The data is normalized wherein it is transformed to lowercase and converted from unicode to ASCII. All non-letter characters are also omitted as part of the normalization process. Normalization is done to define the data in a standard form so as to reduce randomness and increase efficiency. Once the normalization process is completed, we reduce the size of the available input data using two parameters- the length of the sentence (maximum of 10 words) and certain prefixes found in the English language. At the end of this process, we have a standardised limited dataset of English to French pairs.


### Model
In this model, we attempt to perform language translation, specifically English to French, using a Sequence to Sequence network with Attention. For our Sequence to Sequence network, we consider two Recurrent Neural Networks (RNNs), one acting as the encoder and the other the decoder.  The encoder network takes an English sequence as input and and outputs a vector containing a value for each input word. The attention mechanism is implemented on this vector and it highlights relevant parts of the vector. With the help of the attention mechanism, we don't have to encode the full sentence into a fixed-length vector. We can let the decoder learn what to attend based on the input sentence produced so far at each step of the output generation. The decoder takes this weighted combination of all the input states from the attention mechanism and unravels it into a sequence of French words. Since the output of the model depends heavily on the vector generated by the encoder, teacher forcing can be used to guide the decoder so that the model is trained properly. In teacher forcing, the expected output from the training dataset at the current time step is used as input to the next time step instead of the output from the decoder at that time step.<br><br>
For building the model, we will be using GloVe (Global Vectors for Word Representation) embeddings in the embedding layer. GloVe embedding provide an added advantage in comparison to regular word embeddings wherein it takes into consideration not only the local context information of words but also the word's co-occurence in the full corpus. These embeddings relate to the probabilities that two words appear together. Hence, it adds some more meaning into word vectors by considering the relationships between word pair and word pair.<br>

In our model, we are using GloVe embeddings of 300 dimensions and adding 'SOS' and 'EOS' token to the start of the word embeddings. Once the word embeddings are ready and the data has been normalized, we build the weight matrix which is added to the embedding layer of the encoder. This weight matrix is built based on pre-trained weights of the glove vectors.

## Code with Vocabulary
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S10/Seq2Seq_translation_tutorial.ipynb)


Minor modifications in the python code to stretch the solution for English-French Translation as the original model will work for French to English Translations. The modifications are given below.

Chnage the Filter Function now to process the English Translations as Input.
``` python
def filterPair(p):
    return len(p[0].split(' ')) < MAX_LENGTH and \
        len(p[1].split(' ')) < MAX_LENGTH and \
        p[0].startswith(eng_prefixes)
```

Pass without reverse as in this code need only English to French Translations Dataset.
```python
input_lang, output_lang, pairs = prepareData('eng', 'fra')
```

 - **Training Logs**

```
1m 16s (- 17m 55s) (5000 6%) 3.4005
2m 30s (- 16m 21s) (10000 13%) 2.7913
3m 45s (- 15m 0s) (15000 20%) 2.4714
4m 58s (- 13m 42s) (20000 26%) 2.1940
6m 13s (- 12m 26s) (25000 33%) 1.9864
7m 28s (- 11m 12s) (30000 40%) 1.7923
8m 43s (- 9m 58s) (35000 46%) 1.6430
9m 58s (- 8m 43s) (40000 53%) 1.4877
11m 13s (- 7m 29s) (45000 60%) 1.4069
12m 31s (- 6m 15s) (50000 66%) 1.3036
13m 49s (- 5m 1s) (55000 73%) 1.2191
15m 5s (- 3m 46s) (60000 80%) 1.1666
16m 20s (- 2m 30s) (65000 86%) 1.0981
17m 34s (- 1m 15s) (70000 93%) 1.0100
18m 49s (- 0m 0s) (75000 100%) 0.9651
 ```
 
 - **Results**
 
 ```
 > we re almost there .
= encore un petit effort .
< nous y sommes bientot . <EOS>

> i m one of your students .
= je suis de vos eleves .
< je suis l un de vos eleves . <EOS>

> you re smarter than me .
= vous etes plus intelligent que moi .
< vous es plus intelligente que moi . <EOS>

> i m going to go now .
= je vais maintenant y aller .
< je vais aller y aller . <EOS>

> i m really busy tom .
= je suis tres occupee tom .
< je suis vraiment occupe tom . <EOS>

> i m going to be gone soon .
= je vais bientot etre partie .
< je vais etre parti . <EOS>

> i m too old for you .
= je suis trop vieille pour vous .
< je suis trop vieux pour vous . <EOS>

> he is in need of money .
= il a besoin d argent .
< il a besoin d argent . <EOS>

> they re not following me .
= ils ne me suivent pas .
< elles ne me pas pas . <EOS>

> you re all happy .
= vous etes tous heureux .
< vous etes tous contentes . <EOS>
 ```

## Code with Glove Embedding
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/pankaj90382/END-1.0/blob/main/S10/Seq2Seq_translation_tutorial_glove.ipynb)

For GloVe Embedding, TorchText GloVe Embeding can be used easily with 6B Tokens and by default this embedding has 300 features which again can be limited to 50. 100 ..etc.
```python
import torchtext
vectors = torchtext.vocab.GloVe("6B")
```

As the existing solution, have the sos token and eos token at the front of the vocab. Similar to it, there is need to over-ride the vectors of the glove to put in the first place. This can be easily be done with stoi and itos representation of the GloVe Vectors.
``` python
sos_index, eos_index = vectors.stoi['sos'], vectors.stoi['eos']
sos_swap_word, eos_swap_word = vectors.itos[0], vectors.itos[1]
vectors.itos[0], vectors.itos[sos_index] = vectors.itos[sos_index], vectors.itos[0]
vectors.itos[1], vectors.itos[eos_index] = vectors.itos[eos_index], vectors.itos[1]
vectors.stoi[sos_swap_word], vectors.stoi['sos'] = vectors.stoi['sos'], vectors.stoi[sos_swap_word]
vectors.stoi[eos_swap_word], vectors.stoi['eos'] = vectors.stoi['eos'], vectors.stoi[eos_swap_word]
```

Assign it to the current class of input language to over-ride the existing functionality of the vocabulary function. Now it has total number of words more than 400000.
```python
input_lang.word2index = vectors.stoi
input_lang.word2count = { word : 1 for word in vectors.itos }
input_lang.index2word = {i : word for i, word in enumerate (vectors.itos)}
input_lang.n_words = len(vectors.itos)+1
```

In pytorch torchtext, vectors.get_vecs_by_tokens provide the sequence of 300 dimension tensor to dirctly provide the embedding vector. However, to maintain the structural feature as it is, additional numpy vector is created which hold the values of GloVe the corpus.
```python
matrix_len = input_lang.n_words
weights_matrix = np.zeros((matrix_len, 300))
words_found = 0
for i, word in enumerate(input_lang.word2index):
    try: 
        weights_matrix[i] = vectors.get_vecs_by_tokens(word,lower_case_backup=True)
        words_found += 1
    except KeyError:
        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))
```

There is slight modification in embeddingRNN class to superimpose the embedding weights from the GloVe Embeddings weights.
```python
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))
```

Lastly, needs smaller updates in indexesFromSequence function to return the unk token, if the word is not present in the GloVe Corpus.
```python
def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] if word in lang.word2index.keys() else lang.word2index['unk'] for word in sentence.split(' ')]
```

 - **Training Logs**
 
 ```
6m 25s (- 89m 52s) (5000 6%) 3.6031
12m 45s (- 82m 58s) (10000 13%) 3.0094
19m 6s (- 76m 27s) (15000 20%) 2.7094
25m 28s (- 70m 4s) (20000 26%) 2.5165
31m 52s (- 63m 44s) (25000 33%) 2.3402
38m 12s (- 57m 19s) (30000 40%) 2.1463
44m 36s (- 50m 58s) (35000 46%) 1.9839
50m 59s (- 44m 37s) (40000 53%) 1.8386
57m 21s (- 38m 14s) (45000 60%) 1.7170
63m 44s (- 31m 52s) (50000 66%) 1.6044
70m 7s (- 25m 30s) (55000 73%) 1.4923
76m 30s (- 19m 7s) (60000 80%) 1.4245
82m 51s (- 12m 44s) (65000 86%) 1.3228
89m 14s (- 6m 22s) (70000 93%) 1.2303
95m 38s (- 0m 0s) (75000 100%) 1.1617
 ```
 - **Results**

```
> you re not being rational .
= vous n etes pas rationnels .
< tu n es pas rationnelle . <EOS>

> they are russian .
= elles sont russes .
< ils sont russes . <EOS>

> he s likely to be chosen .
= il est probable qu il sera choisi .
< il est probable qu il sera . <EOS>

> i m right behind him .
= je me trouve juste derriere lui .
< je suis juste derriere lui . <EOS>

> you re a wonderful friend .
= vous etes un merveilleux ami .
< tu es une veritable amie . <EOS>

> you re turning thirty .
= vous entrez dans la trentaine .
< tu as de la . . . <EOS>

> you re punctual .
= tu es ponctuel .
< tu es fiable . <EOS>

> i m not selling you my car .
= je refuse de te vendre ma voiture .
< je ne vous vends pas ma voiture . <EOS>

> she is the editor in chief .
= elle est l editeur en chef .
< elle est l editeur la chef . <EOS>

> he s doing it right .
= il le fait correctement .
< il le fait comme il faut . <EOS>

```

## Summary

| model | average loss | total time  |  english words corpus  |  embdedding Dimension
| --- | --- | --- | --- | --- |
| without pre- trained embedding | **0.9651** | 18m 49s |  2803 | 256 |
| with pre- trained GloVe embedding | **1.1617** | 95m 38s |  400001 | 300 |

## Comparison

| input| Vocabulary | GloVe Embedding |
|--|--|--|
|she s five years younger than i am .| elle a cinq ans de moins que moi . <EOS> | elle a six de de que moi . <EOS>
|she s too short .|elle n est trop petit . <EOS>| elle a trop affaire . <EOS>
|i m not scared of dying .|je ne crains pas mourir mourir . <EOS>| je ne ai pas de de mourir . <EOS>
|he s a talented young talented .|c est un jeune de talentueux . <EOS>| c est un jeune jeune talentueux . <EOS>

## Refrences
 - [Glove Embedding](https://leakyrelu.com/2019/10/18/using-glove-word-embeddings-with-seq2seq-encoder-decoder-in-pytorch/)
