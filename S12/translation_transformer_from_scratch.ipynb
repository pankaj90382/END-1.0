{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"translation_transformer_from_scratch.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/8cdd9a659f7d22e15eb4a689206e4b6b/translation_transformer.ipynb","timestamp":1627488633116}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6x5yb4Yi4XKC","executionInfo":{"status":"ok","timestamp":1627558293718,"user_tz":-330,"elapsed":12,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k9MXGTSn4XKF"},"source":["\n","Language Translation with nn.Transformer and torchtext\n","======================================================\n","\n","This tutorial shows, how to train a translation model from scratch using\n","Transformer. We will be using `Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>`__ \n","dataset to train a German to English translation model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MB1T5AM24XKF"},"source":["Data Sourcing and Processing\n","----------------------------\n","\n","`torchtext library <https://pytorch.org/text/stable/>`__ has utilities for creating datasets that can be easily\n","iterated through for the purposes of creating a language translation\n","model. In this example, we show how to use torchtext's inbuilt datasets, \n","tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n","`Multi30k dataset from torchtext library <https://pytorch.org/text/stable/datasets.html#multi30k>`__\n","that yields a pair of source-target raw sentences. \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lG1Otr3jPatD","executionInfo":{"status":"ok","timestamp":1627558308181,"user_tz":-330,"elapsed":14471,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}},"outputId":"8f3c965c-0745-44a3-c634-4251fb80a36a"},"source":["%%bash\n","python -m spacy download en\n","python -m spacy download de"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n","Collecting de_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.2.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n","Building wheels for collected packages: de-core-news-sm\n","  Building wheel for de-core-news-sm (setup.py): started\n","  Building wheel for de-core-news-sm (setup.py): finished with status 'done'\n","  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=c2e0a216d30bfc53665b50307afc6b3b01ae93826fcc5eceb57991ec43d871a7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-pum8pbqc/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n","Successfully built de-core-news-sm\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/de\n","You can now load the model via spacy.load('de')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ckdn6l2n4XKG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627558320945,"user_tz":-330,"elapsed":12776,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}},"outputId":"f3b4928c-156b-40c0-a071-843be0ec4a4a"},"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import Multi30k\n","from typing import Iterable, List\n","\n","\n","SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}\n","\n","\n","# Create source and target language tokenizer. Make sure to install the dependencies.\n","# pip install -U spacy\n","# python -m spacy download en_core_web_sm\n","# python -m spacy download de_core_news_sm\n","token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en')\n","\n","\n","# helper function to yield list of tokens\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n"," \n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator \n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    # Create torchtext's Vocab object \n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","# Set UNK_IDX as the default index. This index is returned when the token is not found. \n","# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","  vocab_transform[ln].set_default_index(UNK_IDX)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 964kB/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oJJ6IIbr4XKH"},"source":["Seq2Seq Network using Transformer\n","---------------------------------\n","\n","Transformer is a Seq2Seq model introduced in `“Attention is all you\n","need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n","paper for solving machine translation tasks. \n","Below, we will create a Seq2Seq network that uses Transformer. The network\n","consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n","into corresponding tensor of input embeddings. These embedding are further augmented with positional\n","encodings to provide position information of input tokens to the model. The second part is the \n","actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model. \n","Finally, the output of Transformer model is passed through linear layer\n","that give un-normalized probabilities for each token in the target language. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"CL3VDKQcTT44","executionInfo":{"status":"ok","timestamp":1627558321557,"user_tz":-330,"elapsed":623,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["import torch\n","import torch.nn as nn\n","\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length,\n","    ):\n","\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                TransformerBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout,\n","                    forward_expansion=forward_expansion,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out = self.dropout(\n","            (self.word_embedding(x) + self.position_embedding(positions))\n","        )\n","\n","        # In the Encoder the query, key, value are all the same, it's in the\n","        # decoder this will change. This might look a bit odd in this case.\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.attention = SelfAttention(embed_size, heads=heads)\n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.transformer_block(value, key, query, src_mask)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super(Decoder, self).__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","\n","        return out\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        trg_pad_idx,\n","        embed_size=512,\n","        num_layers=6,\n","        forward_expansion=4,\n","        heads=8,\n","        dropout=0,\n","        device=\"cpu\",\n","        max_length=100,\n","    ):\n","\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_src = self.encoder(src, src_mask)\n","        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        return out"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwyGfl1l4XKI","executionInfo":{"status":"ok","timestamp":1627558321559,"user_tz":-330,"elapsed":11,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbIrKIdp4XKN"},"source":["Let's now define the parameters of our model and instantiate the same. Below, we also \n","define our loss function which is the cross-entropy loss and the optmizer used for training.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PDDKPzHW4XKN","executionInfo":{"status":"ok","timestamp":1627558333807,"user_tz":-330,"elapsed":12256,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8\n","Forward_Expansion=4\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = Transformer(num_layers=NUM_ENCODER_LAYERS, embed_size = EMB_SIZE, src_pad_idx = PAD_IDX, trg_pad_idx= PAD_IDX,\n","                                 heads=NHEAD, src_vocab_size = SRC_VOCAB_SIZE, trg_vocab_size = TGT_VOCAB_SIZE, dropout=0.1, device=DEVICE)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_mbIVXm4XKO"},"source":["Collation\n","---------\n","\n","As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n","We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n","defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n","can be fed directly into our model.   \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AjCh8sp94XKO","executionInfo":{"status":"ok","timestamp":1627558333808,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]), \n","                      torch.tensor(token_ids), \n","                      torch.tensor([EOS_IDX])))\n","\n","# src and tgt language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n","                                               vocab_transform[ln], #Numericalization\n","                                               tensor_transform) # Add BOS/EOS and create tensor\n","\n","\n","# function to collate data samples into batch tesors\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n","    return src_batch, tgt_batch"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3KunDRa4XKP"},"source":["Let's define training and evaluation loop that will be called for each \n","epoch.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_l98bRRe4XKP","executionInfo":{"status":"ok","timestamp":1627558333809,"user_tz":-330,"elapsed":30,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}}},"source":["from torch.utils.data import DataLoader\n","\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","    \n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[: , :-1]\n","\n","        logits = model(src, tgt_input)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[:, 1:]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[: , :-1]\n","\n","        logits = model(src, tgt_input)\n","        \n","        tgt_out = tgt[:, 1:]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dS5Y2G9D4XKP"},"source":["Now we have all the ingredients to train our model. Let's do it!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"B5lqdFKR4XKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627559229021,"user_tz":-330,"elapsed":895239,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}},"outputId":"4253f8ff-b341-4c2b-9fff-17ade6623595"},"source":["from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 112kB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1, Train loss: 5.496, Val loss: 4.562, Epoch time = 47.704s\n","Epoch: 2, Train loss: 4.276, Val loss: 3.952, Epoch time = 49.146s\n","Epoch: 3, Train loss: 3.850, Val loss: 3.683, Epoch time = 48.867s\n","Epoch: 4, Train loss: 3.590, Val loss: 3.490, Epoch time = 49.145s\n","Epoch: 5, Train loss: 3.379, Val loss: 3.305, Epoch time = 48.945s\n","Epoch: 6, Train loss: 3.175, Val loss: 3.149, Epoch time = 49.076s\n","Epoch: 7, Train loss: 2.992, Val loss: 2.975, Epoch time = 48.966s\n","Epoch: 8, Train loss: 2.817, Val loss: 2.819, Epoch time = 49.118s\n","Epoch: 9, Train loss: 2.656, Val loss: 2.684, Epoch time = 49.009s\n","Epoch: 10, Train loss: 2.509, Val loss: 2.562, Epoch time = 48.945s\n","Epoch: 11, Train loss: 2.381, Val loss: 2.466, Epoch time = 49.049s\n","Epoch: 12, Train loss: 2.265, Val loss: 2.381, Epoch time = 49.079s\n","Epoch: 13, Train loss: 2.154, Val loss: 2.308, Epoch time = 49.017s\n","Epoch: 14, Train loss: 2.059, Val loss: 2.246, Epoch time = 49.173s\n","Epoch: 15, Train loss: 1.973, Val loss: 2.206, Epoch time = 49.284s\n","Epoch: 16, Train loss: 1.896, Val loss: 2.159, Epoch time = 49.112s\n","Epoch: 17, Train loss: 1.815, Val loss: 2.141, Epoch time = 49.069s\n","Epoch: 18, Train loss: 1.744, Val loss: 2.106, Epoch time = 48.967s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"az5c9K6H4XKQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627561385786,"user_tz":-330,"elapsed":415,"user":{"displayName":"Pankaj Goyal","photoUrl":"","userId":"05793163187412977076"}},"outputId":"85fd8cff-6d88-4cd6-cf46-0e230a9803af"},"source":["import numpy as np\n","for i in np.random.randint(0,32, 5):\n","  val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","  val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","  src_sent_i = next(iter(val_dataloader))[0][i,:]\n","  trg_sent_i = next(iter(val_dataloader))[1][i,:]\n","  stop_ind_src = (src_sent_i==3).nonzero()[0].item() # stop when <eos> token is found\n","  stop_ind_trg = (trg_sent_i==3).nonzero()[0].item() # stop when <eos> token is found\n","  src_sent_tok = [vocab_transform['de'].lookup_token(word_i) for word_i in src_sent_i[:stop_ind_src]]\n","  trg_sent_tok = [vocab_transform['en'].lookup_token(word_i) for word_i in trg_sent_i[:stop_ind_trg]]\n","  src_sent = \" \".join(src_sent_tok[1:]) # skip the initial <bos> token\n","  trg_sent = \" \".join(trg_sent_tok[1:]) # skip the initial <bos> token\n","  src_sent_tensor = src_sent_i.clone().detach().unsqueeze(0).to(DEVICE)   \n","  trg_sent_tensor = trg_sent_i.clone().detach().unsqueeze(0).to(DEVICE) \n","  with torch.no_grad():\n","        output = transformer(src_sent_tensor, trg_sent_tensor)\n","        out = output.squeeze(0)\n","        out = torch.argmax(out,dim=1)\n","        if any(out==3) == False: # if <eos> token is not found\n","            stop_ind_pred = len(out)   # use complete sentence\n","        else:\n","            stop_ind_pred = (out==3).nonzero()[0].item() # stop when <eos> token is found\n","        pred_sent_tok = [vocab_transform['en'].lookup_token(word_i) for word_i in out]\n","        pred_sent = \" \".join(pred_sent_tok[:stop_ind_pred])\n","        start = \"\\033[1m\"\n","        end = \"\\033[0;0m\"\n","        print(f'{start}Source Sentence: {end}{src_sent}')\n","        print(f'{start}Target Sentence: {end}{trg_sent}')\n","        print(f'{start}Translated Sentence: {end}{pred_sent}')\n","        print()"],"execution_count":57,"outputs":[{"output_type":"stream","text":["\u001b[1mSource Sentence: \u001b[0;0mEin kleines Kind steht allein auf einem zerklüfteten Felsen .\n","\u001b[1mTarget Sentence: \u001b[0;0mLittle boy in <unk> crawling on brown floor\n","\u001b[1mTranslated Sentence: \u001b[0;0mA child standing a gear on a rock . . . . . . . . . . . . . . . . . . . . . . .\n","\n","\u001b[1mSource Sentence: \u001b[0;0mEine lächelnde Frau mit einem pfirsichfarbenen Trägershirt hält ein Mountainbike\n","\u001b[1mTarget Sentence: \u001b[0;0mA group of men , women , and children , all wearing hats , talk on the beach .\n","\u001b[1mTranslated Sentence: \u001b[0;0mA smiling of woman with holding holding holding holding holding holding holding a on holding . a mountain .\n","\n","\u001b[1mSource Sentence: \u001b[0;0mEin Junge mit Kopfhörern sitzt auf den Schultern einer Frau .\n","\u001b[1mTarget Sentence: \u001b[0;0mPeople are being photographed while mountain climbing or hiking .\n","\u001b[1mTranslated Sentence: \u001b[0;0mA with wearing asleep on sitting on on wearing .\n","\n","\u001b[1mSource Sentence: \u001b[0;0mEine Person überquert die Straße und <unk> dabei die <unk> Farbe .\n","\u001b[1mTarget Sentence: \u001b[0;0mTwo men talk to each other behind an organ while a lone woman stands to their right .\n","\u001b[1mTranslated Sentence: \u001b[0;0mA person crossing and the other in the escalator . . street street . . . bicycle .\n","\n","\u001b[1mSource Sentence: \u001b[0;0mEin Mann mit beginnender Glatze , der eine rote Rettungsweste trägt , sitzt in einem kleinen Boot .\n","\u001b[1mTarget Sentence: \u001b[0;0mWomen in beach bikinis play volleyball in the sunny weather .\n","\u001b[1mTranslated Sentence: \u001b[0;0mA wearing tan , , a in a red area .\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nWbOg6zD4XKQ"},"source":["References\n","----------\n","\n","1. Attention is all you need paper.\n","   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n","2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n","\n"]}]}